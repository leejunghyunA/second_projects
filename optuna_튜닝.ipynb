{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f72b779",
   "metadata": {},
   "source": [
    "```\n",
    "Optuna란 하이퍼파라미터 최적화 태스크를 자동화해주는 프레임워크로, 다음과 같은 장점이 있다.\n",
    "\n",
    "거의 모든 ML/DL 프레임워크에서 사용 가능한 넓은 범용성을 가지고 있다.\n",
    "간단하고 빠르다.\n",
    "최신 동향의 다양한 최적화 알고리즘을 갖추고 있다.\n",
    "병렬 처리가 가능하다.\n",
    "간단한 메소드로 시각화가 가능\n",
    "\n",
    "Optuna를 이해하기 위해서는 다음의 용어에 익숙해져야 한다.\n",
    "Study: 목적 함수에 기반한 최적화\n",
    "Trial: 목적함수 시행\n",
    "쉽게 말해 study는 최적화를 하는 과정이고, trial은 다양한 조합으로 목적함수를 시행하는 횟수를 뜻한다.\n",
    "Study의 목적은 여러 번의 trial을 거쳐 최적의 하이퍼파라미터 조합을 찾는 것이라고 할 수 있겠다.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573cb34c",
   "metadata": {},
   "source": [
    "![nn](optuna.png)\n",
    "## 복사는 밑에 코드블럭에서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674e0da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integer parameter : A uniform distribution on integers.\n",
    "# n_estimators = trial.suggest_int('n_estimators',100,500)\n",
    "\n",
    "# Categorical parameter : A categorical distribution.\n",
    "# criterion = trial.suggest_categorical('criterion' ,['gini', 'entropy'])\n",
    "\n",
    "# Uniform parameter : A uniform distribution in linear domain.\n",
    "# subsample = trial.suggest_uniform('subsample' ,0.2,0.8)\n",
    "\n",
    "# Discrete-uniform parameter : A discretized uniform distribution in linear domain.\n",
    "# max_features = trial.suggest_discrete_uniform('max_features', 0.05,1,0.05)\n",
    "\n",
    "# Loguniform parameter : A uniform distribution in log domain. ** learning_rate = trial.sugget_loguniform('learning_rate' : 1e-6, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bca687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110c92df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= \n",
    "y=  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2f40f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test= train_test_split(X, y, test_size=0.3, random_state=156) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecc7517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "# 1. 최소화/최대화할 목적함수 정의\n",
    "def objective(trial):\n",
    "    iris = sklearn.datasets.load_iris()\n",
    "    x, y = iris.data, iris.target\n",
    "\n",
    "# 2. trial object로 하이퍼파라미터 값 추천\n",
    "# 다양한 분류모델을 설정해서 비교할 수 있다.\n",
    "    classifier_name = trial.suggest_categorical('classifier', ['SVC', 'LogisticRegression','DecisionTreeClassifier','GradientBoostingClassifier','KNeighborsClassifier','LGBMClassifier','RandomForest'])\n",
    "    #분류 모델이 SVC일 때\n",
    "    if classifier_name == 'SVC':\n",
    "        svc_c = trial.suggest_loguniform('svc_c', 1e-10, 1e10)\n",
    "        classifier_obj = svm.SVC(C=svc_c, gamma='auto')\n",
    "    \n",
    "    #LogisticRegression\n",
    "    elif classifier_name == 'LogisticRegression':       \n",
    "        penalty = trial.suggest_categorical('penalty', ['l1', 'l2', 'elasticnet'])     # l1 is Lasso, l2 is Ridge\n",
    "        C = trial.suggest_uniform('C', 0.00002,1)\n",
    "        classifier_obj =  LogisticRegression(penalty=penalty, C=C, solver='liblinear')\n",
    "    \n",
    "    #DecisionTreeClassifier\n",
    "    elif classifier_name == 'DecisionTreeClassifier':       \n",
    "        max_depth = trial.suggest_int('max_depth',2,20)     \n",
    "        min_samples_leaf = trial.suggest_discrete_uniform('min_samples_leaf',5,100,5)\n",
    "        criterion = trial.suggest_categorical('criterion', [\"gini\", \"entropy\"]) \n",
    "    \n",
    "        classifier_obj =  DecisionTreeClassifier(max_depth=max_depth, min_samples_leaf=min_samples_leaf, criterion=criterion)\n",
    "    \n",
    "    #GradientBoostingClassifier\n",
    "    elif classifier_name == 'GradientBoostingClassifier':\n",
    "        max_depth = trial.suggest_int('max_depth',2,20)\n",
    "        learning_rate = trial.suggest_discrete_uniform('learning_rate',0.01,0.2,0.005)\n",
    "        min_samples_split = trial.suggest_discrete_uniform('min_samples_split',0.1,0.5,0.05)\n",
    "        min_samples_leaf = trial.suggest_discrete_uniform('min_samples_leaf',0.1,0.5,0.05)\n",
    "        max_features = trial.suggest_categorical('max_features', [\"log2\",\"sqrt\"])\n",
    "        criterion = trial.suggest_categorical('criterion', [\"friedman_mse\",  \"mae\"]) #어 얘는 손실함수라 밑에 따로해야할수도.. \n",
    "        subsample = trial.suggest_categorical('subsample', [0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0])\n",
    "        \n",
    "        classifier_obj =  GradientBoostingClassifier(loss='deviance',n_estimators=10, max_depth=max_depth, learning_rate=learning_rate,min_samples_split=min_samples_split,\n",
    "                                                    min_samples_leaf=min_samples_leaf,max_features=max_features,criterion=criterion,subsample=subsample)\n",
    "        \n",
    "    #KNeighborsClassifier    \n",
    "    elif classifier_name == 'KNeighborsClassifier':\n",
    "        n_neighbors = trial.suggest_discrete_uniform('n_neighbors',1,20,1)\n",
    "        weights = trial.suggest_categorical('weights', [\"uniform\", \"distance\"])\n",
    "        metric = trial.suggest_categorical('metric', ['euclidean', 'manhattan', 'minkowski'])\n",
    "        \n",
    "        classifier_obj =  KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights, metric=metric)\n",
    "    \n",
    "    #LGBMClassifier    \n",
    "    elif classifier_name == 'LGBMClassifier':\n",
    "        \n",
    "        num_leaves = trial.suggest_discrete_uniform('num_leaves',20,100,10)\n",
    "        min_child_samples = trial.suggest_discrete_uniform('min_child_samples',5,20,5)\n",
    "        max_depth = trial.suggest_categorical('max_depth', [-1,5,10,20])\n",
    "        learning_rate = trial.suggest_discrete_uniform('learning_rate',0.05,0.5,0.05)\n",
    "        reg_alpha = trial.suggest_discrete_uniform('reg_alpha',0,0.05,0.01)\n",
    "        \n",
    "        classifier_obj =  LGBMClassifier(num_leaves=num_leaves,min_child_samples=min_child_samples,max_depth=max_depth,learning_rate=learning_rate,reg_alpha=reg_alpha)\n",
    "    \n",
    "    #분류모델이 랜덤포레스트일 때\n",
    "    else:\n",
    "        rf_max_depth = int(trial.suggest_loguniform('rf_max_depth', 2, 32))\n",
    "        classifier_obj = RandomForestClassifier(max_depth=rf_max_depth, n_estimators=10)\n",
    "    \n",
    "    accuracy = cross_val_score(classifier_obj,X_train, y_train, cv = 4).mean()\n",
    "    return accuracy\n",
    "\n",
    "# 3. study 오브젝트 생성하고 목적함수 최적화하는 단계\n",
    "# 여기서는 목적함수를 정확도로 설정했기 때문에 최대화를 목표로 하고 있지만, 손실함수의 경우 direction='minimize'로 설정\n",
    "study = optuna.create_study(direction='maximize')\n",
    "# 반복 시행 횟수(trial)는 200번으로\n",
    "study.optimize(objective, n_trials=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4050f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시행된 trial 중 최적의 하이퍼파라미터 반환하는 메소드\n",
    "print(study.best_trial.params)\n",
    "\n",
    "# 시행된 trial 중 가장 높은 값 반환하는 메소드\n",
    "optuna_acc = study.best_trial.value\n",
    "print(optuna_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
